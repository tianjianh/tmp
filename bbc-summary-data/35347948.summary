[SN]URL[SN]
http://web.archive.org/web/20160119123438/http://www.bbc.co.uk/news/uk-politics-35347948

[SN]TITLE[SN]
Election polling errors blamed on 'unrepresentative' samples

[SN]FIRST-SENTENCE[SN]
The failure of pollsters to forecast the outcome of the general election was largely due to "unrepresentative" poll samples, an inquiry has found.

[SN]RESTBODY[SN]
The polling industry came under fire for predicting a virtual dead heat when the Conservatives ultimately went on to outpoll Labour by 36.9% to 30.4%.
A panel of experts has concluded this was due to Tory voters being under-represented in phone and online polls.
But it said it was impossible to say whether "late swing" was also a factor.
The majority of polls taken during last year's five-week election campaign suggested that the election was too close to call and that David Cameron's Conservatives and Ed Miliband's Labour remained neck-and-neck.
Laura Kuenssberg: Why were the polls so wrong?
This led to speculation that Labour could be the largest party in a hung parliament and could potentially have to rely on SNP support to govern.
But, as it turned out, the Conservatives secured an overall majority in May for the first time since 1992, winning 99 more seats than Labour, their margin of victory taking nearly all commentators by surprise.
The mismatch between pre-election polls and the actual outcome prompted the polling industry to launch an independent inquiry into the accuracy of its polls, the reasons for any inaccuracies and how polls were analysed and reported.
An interim report by the panel of academics and statisticians found that the way in which people were recruited to take part in polls asking about their likely voting intentions had resulted in "systematic over-representation of Labour voters and under-representation of Conservative voters".
These oversights, it found, had resulted in a "statistical consensus".
This, it said, was borne out by polls taken after the general election by the British Election Study and the British Social Attitudes Survey, which produced a much more accurate assessment of the Conservatives' lead over Labour.
It downplayed other potential explanations such as misreporting of voter turnout, problems with question wording or how overseas, postal or unregistered voters were treated in the polls, concluding they could only have made a modest contribution to the discrepancies.
It reached no definitive verdict on whether a last-minute swing to the Conservatives was a factor in the poll confusion, as has been suggested by former Lib Dem leader Nick Clegg who has ascribed it to voters' fears about a hung parliament and the possibility of a Labour-SNP tie-up.
The experts said the evidence was "inconsistent" and that if it did happen its effect was likely to have been modest.
However, the panel said it could not rule out the possibility of "herding" - where firms configured their polls in a way that caused them to deviate less than could have been expected from others given the sample sizes. But it stressed that did not imply malpractice on behalf of the firms concerned.
Analysis by the BBC's political editor Laura Kuenssberg
But for all that the consequences of the 2015 polls were many and various, the reasons, as outlined today by academics, appear remarkably simple.
Pollsters didn't ask enough of the right people how they planned to vote. Proportionately they asked too many likely Labour voters, and not enough likely Conservatives
Politics is not a precise science and predicting how people will vote will still be a worthwhile endeavour. Political parties, journalists, and the public of course would be foolish to ignore them. But the memories and embarrassment for the polling industry of 2015 will take time to fade.
Read Laura's blog on how the pundits and pollsters got it wrong
Professor Patrick Sturgis, Director of the National Centre for Research Methods at the University of Southampton and chair of the panel, said the way that polling firms conducted their research needed to be looked at.
"They don't collect samples in the way the Office for National Statistics does by taking random samples and keeping knocking on doors until they have got enough people," he told the BBC.
"What they do is get anyone they can and try and match them to the population in terms of some of the things we know it terms of how the population looks. That approach is perfectly fine in many cases but sometimes it goes wrong."
Although the panel will make a series of recommendations to pollsters, Mr Sturgis said there was "no silver bullet" to ensure the errors didn't happen again and that "we cannot rule out another polling inquiry in 20 years time".
Pollsters criticised for their performance have pointed to the fact that they accurately predicted the stellar performance of the SNP in Scotland - which won 56 out of 59 seats - and the fact that the Lib Dems would get less than 10% of the vote and be overtaken by UKIP.
An exit poll conducted on polling day by NOP/MORI for the BBC, ITV and Sky - whose results were announced just after polls closed - found that the Conservatives would comfortably be the largest party, nearly 80 seats ahead of Labour.
The poll also underestimated the Conservative performance, giving them 15 fewer seats than they ended up with.
